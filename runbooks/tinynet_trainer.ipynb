{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd442da46f3",
   "metadata": {},
   "source": [
    "# TinyNet Trainer: From Overfitting to Generalization\n",
    "\n",
    "**Prerequisites:** Complete `tinynet_discovery.ipynb` first\n",
    "\n",
    "In the Discovery notebook, TinyNet's problem was clear:\n",
    "- Trained on 4 perfect examples\n",
    "- Memorized instead of learning patterns\n",
    "- Failed on stress tests\n",
    "\n",
    "**Today's Mission:**\n",
    "1. Add noise to training data\n",
    "2. Compare Sigmoid vs ReLU\n",
    "3. Track experiments with Weights & Biases\n",
    "4. Build a model that generalizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedd34f5761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install torch torchvision matplotlib seaborn numpy wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d036dd693ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import wandb\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Hardware acceleration\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# wandb setup: online if logged in, offline otherwise\n",
    "if wandb.api.api_key or os.getenv(\"WANDB_API_KEY\"):\n",
    "    wandb_mode = \"online\"\n",
    "    print(\"\\nwandb: Online mode - experiments syncing to cloud\")\n",
    "else:\n",
    "    wandb_mode = \"offline\"\n",
    "    print(\"\\nwandb: Offline mode - experiments saved locally\")\n",
    "    print(\"To enable cloud sync: run 'wandb login' in terminal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407bcfaadc9",
   "metadata": {},
   "source": [
    "## Part 1: The Noise Solution\n",
    "\n",
    "**Discovery notebook's problem:** 4 perfect examples → memorization, not learning\n",
    "\n",
    "**The fix:** Add random variation to training data\n",
    "- Instead of `[1.0, 1.0, 0.0, 0.0]`\n",
    "- Train on `[0.95, 1.0, 0.12, 0.05]`, `[0.88, 0.92, 0.0, 0.08]`, etc.\n",
    "- Forces the model to learn patterns, not memorize specific values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091d8672ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noisy_batch(batch_size=32, noise_level=0.1):\n",
    "    \"\"\"\n",
    "    Generate batch of noisy horizontal and vertical lines.\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Number of examples to generate\n",
    "        noise_level: How much random variation to add (e.g., 0.1 = ±5%)\n",
    "    \n",
    "    Returns:\n",
    "        inputs: Tensor of shape [batch_size, 4]\n",
    "        targets: Tensor of shape [batch_size, 2]\n",
    "    \"\"\"\n",
    "    # Base patterns\n",
    "    h_bases = [\n",
    "        torch.tensor([1.0, 1.0, 0.0, 0.0]),  # Horizontal top\n",
    "        torch.tensor([0.0, 0.0, 1.0, 1.0])   # Horizontal bottom\n",
    "    ]\n",
    "    v_bases = [\n",
    "        torch.tensor([1.0, 0.0, 1.0, 0.0]),  # Vertical left\n",
    "        torch.tensor([0.0, 1.0, 0.0, 1.0])   # Vertical right\n",
    "    ]\n",
    "    \n",
    "    inputs, targets = [], []\n",
    "    \n",
    "    for _ in range(batch_size // 2):\n",
    "        # Add noise: uniform random in range [-noise_level/2, +noise_level/2]\n",
    "        h_noise = (torch.rand(4) * noise_level) - (noise_level / 2)\n",
    "        v_noise = (torch.rand(4) * noise_level) - (noise_level / 2)\n",
    "        \n",
    "        # Apply noise and clamp to [0, 1]\n",
    "        h_noisy = torch.clamp(random.choice(h_bases) + h_noise, 0, 1)\n",
    "        v_noisy = torch.clamp(random.choice(v_bases) + v_noise, 0, 1)\n",
    "        \n",
    "        inputs.append(h_noisy)\n",
    "        targets.append(torch.tensor([1.0, 0.0]))  # Horizontal\n",
    "        \n",
    "        inputs.append(v_noisy)\n",
    "        targets.append(torch.tensor([0.0, 1.0]))  # Vertical\n",
    "    \n",
    "    return torch.stack(inputs).to(device), torch.stack(targets).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9953c17f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and visualize some noisy examples\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "inputs, _ = get_noisy_batch(batch_size=8, noise_level=0.6)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 4.5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx in range(8):\n",
    "    pixels = inputs[idx].cpu().numpy()\n",
    "    grid = pixels.reshape(2, 2)\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Set up the plot\n",
    "    ax.set_xlim(-0.5, 1.5)\n",
    "    ax.set_ylim(-0.5, 1.5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.invert_yaxis()  # Put origin at top-left\n",
    "    ax.set_title(f'Example {idx+1}', fontsize=10)\n",
    "    \n",
    "    # Remove axis ticks\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    # Draw circles for each pixel\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            value = grid[i, j]\n",
    "            # Circle filled based on brightness (use grayscale)\n",
    "            circle = patches.Circle((j, i), radius=0.3, \n",
    "                                   facecolor=str(1-value),  # Inverse for grayscale (0=black, 1=white)\n",
    "                                   edgecolor='black', linewidth=2)\n",
    "            ax.add_patch(circle)\n",
    "            \n",
    "            # Add pixel value as text\n",
    "            ax.text(j, i, f'{value:.3f}', ha='center', va='center', \n",
    "                   fontsize=7, color='red' if value > 0.5 else 'blue', weight='bold')\n",
    "    \n",
    "    # Draw outer border (thick, dark)\n",
    "    outer_rect = patches.Rectangle((-0.5, -0.5), 2, 2, \n",
    "                                  linewidth=3, edgecolor='black', \n",
    "                                  facecolor='none')\n",
    "    ax.add_patch(outer_rect)\n",
    "    \n",
    "    # Draw inner grid lines (thin, light gray)\n",
    "    ax.plot([0.5, 0.5], [-0.5, 1.5], color='gray', linewidth=1, alpha=0.5)\n",
    "    ax.plot([-0.5, 1.5], [0.5, 0.5], color='gray', linewidth=1, alpha=0.5)\n",
    "\n",
    "plt.suptitle('Noisy Training Examples (noise_level=0.6)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: Values are no longer perfect 0.0 or 1.0\")\n",
    "print(\"But the patterns (horizontal/vertical) are still visible!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61c44df705",
   "metadata": {},
   "source": [
    "### How Much Noise?\n",
    "\n",
    "- **Too little (0.05)**: Model still memorizes, barely better than Book notebook\n",
    "- **Visualization (0.6)**: Perfect for demonstrating the concept - noise is obvious, patterns still clear\n",
    "- **Training (0.3)**: Sweet spot for robust learning without overwhelming the signal\n",
    "- **Too much (0.8+)**: Pattern becomes ambiguous, model can't learn reliably\n",
    "\n",
    "We'll use **0.6 for visualization** (so you can see the noise clearly) and **0.3 for training** (optimal learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e3e106784",
   "metadata": {},
   "source": [
    "## Part 2: Experiment 1 - Sigmoid + Noise\n",
    "\n",
    "Let's keep the Sigmoid activation from the Book notebook, but train on noisy data.\n",
    "\n",
    "**Hypothesis:** With enough diverse examples, even Sigmoid should generalize better than the Book notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336577ef2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyNet_Sigmoid(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TinyNet_Sigmoid, self).__init__()\n",
    "        self.layer1 = nn.Linear(4, 3)\n",
    "        self.layer2 = nn.Linear(3, 2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.layer1(x))\n",
    "        x = self.sigmoid(self.layer2(x))\n",
    "        return x\n",
    "\n",
    "model_sigmoid = TinyNet_Sigmoid().to(device)\n",
    "print(f\"Sigmoid model parameters: {sum(p.numel() for p in model_sigmoid.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac38f3ea607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb for Sigmoid experiment\n",
    "wandb.init(\n",
    "    project=\"tinynet-trainer\",\n",
    "    name=f\"sigmoid-noisy-training-{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "    tags=[\"v1\", \"sigmoid\", \"sgd\"],\n",
    "    mode=wandb_mode,\n",
    "    config={\n",
    "        \"architecture\": \"4-3-2\",\n",
    "        \"activation\": \"sigmoid\",\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"noise_level\": 0.3,\n",
    "        \"batch_size\": 64,\n",
    "        \"epochs\": 200,\n",
    "        \"device\": str(device)\n",
    "    }\n",
    ")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer_sigmoid = torch.optim.SGD(model_sigmoid.parameters(), lr=0.1)\n",
    "loss_history_sigmoid = []\n",
    "\n",
    "print(\"Training Sigmoid model with noisy data...\")\n",
    "for epoch in range(200):\n",
    "    inputs, labels = get_noisy_batch(batch_size=64, noise_level=0.3)\n",
    "    \n",
    "    outputs = model_sigmoid(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    optimizer_sigmoid.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_sigmoid.step()\n",
    "    \n",
    "    loss_history_sigmoid.append(loss.item())\n",
    "    \n",
    "    # Log to wandb\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": loss.item()})\n",
    "    \n",
    "    if epoch % 40 == 0:\n",
    "        print(f\"Epoch {epoch:03d} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Finish this wandb run\n",
    "wandb.finish()\n",
    "\n",
    "plt.plot(loss_history_sigmoid)\n",
    "plt.title(\"Sigmoid Training with Noisy Data\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a884764f62e",
   "metadata": {},
   "source": [
    "## Part 3: The ReLU Upgrade\n",
    "\n",
    "**Why ReLU?**\n",
    "- No saturation (unlike Sigmoid which gets stuck at 0/1)\n",
    "- Better gradient flow → faster training\n",
    "- Standard in modern networks\n",
    "\n",
    "**Strategy:** ReLU on hidden layer, Sigmoid on output (for probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a078075bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyNet_ReLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TinyNet_ReLU, self).__init__()\n",
    "        self.layer1 = nn.Linear(4, 3)\n",
    "        self.layer2 = nn.Linear(3, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()  # Only on output for probabilities\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))      # ReLU on hidden layer\n",
    "        x = self.sigmoid(self.layer2(x))   # Sigmoid on output layer\n",
    "        return x\n",
    "\n",
    "model_relu = TinyNet_ReLU().to(device)\n",
    "print(f\"ReLU model parameters: {sum(p.numel() for p in model_relu.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74270d4d18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb for ReLU experiment\n",
    "wandb.init(\n",
    "    project=\"tinynet-trainer\",\n",
    "    name=f\"relu-v1-{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "    tags=[\"v1\", \"relu\", \"sgd\"],\n",
    "    mode=wandb_mode,\n",
    "    config={\n",
    "        \"architecture\": \"4-3-2\",\n",
    "        \"activation\": \"relu\",\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"noise_level\": 0.3,\n",
    "        \"batch_size\": 64,\n",
    "        \"epochs\": 200,\n",
    "        \"device\": str(device)\n",
    "    }\n",
    ")\n",
    "\n",
    "optimizer_relu = torch.optim.SGD(model_relu.parameters(), lr=0.1)\n",
    "loss_history_relu = []\n",
    "\n",
    "print(\"Training ReLU model with noisy data...\")\n",
    "for epoch in range(200):\n",
    "    inputs, labels = get_noisy_batch(batch_size=64, noise_level=0.3)\n",
    "    \n",
    "    outputs = model_relu(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    optimizer_relu.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_relu.step()\n",
    "    \n",
    "    loss_history_relu.append(loss.item())\n",
    "    \n",
    "    # Log to wandb\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": loss.item()})\n",
    "    \n",
    "    if epoch % 40 == 0:\n",
    "        print(f\"Epoch {epoch:03d} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Finish this wandb run\n",
    "wandb.finish()\n",
    "\n",
    "plt.plot(loss_history_relu)\n",
    "plt.title(\"ReLU Training with Noisy Data\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e938aab8a",
   "metadata": {},
   "source": [
    "### Initial Observations\n",
    "\n",
    "Notice anything different about the training curves?\n",
    "\n",
    "ReLU typically converges faster and to a lower loss than Sigmoid. Let's compare them side-by-side."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ebd79bdc0",
   "metadata": {},
   "source": [
    "## Part 4: Side-by-Side Comparison\n",
    "\n",
    "Now let's compare the two approaches directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4adec5628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare loss curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(loss_history_sigmoid, label='Sigmoid', linewidth=2, alpha=0.8)\n",
    "plt.plot(loss_history_relu, label='ReLU', linewidth=2, alpha=0.8)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Convergence: Sigmoid vs ReLU')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final loss - Sigmoid: {loss_history_sigmoid[-1]:.4f}\")\n",
    "print(f\"Final loss - ReLU: {loss_history_relu[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8a60a5992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare learned weights (heatmaps)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "# Sigmoid weights\n",
    "sigmoid_weights = model_sigmoid.layer1.weight.data.cpu().numpy()\n",
    "for i in range(3):\n",
    "    sns.heatmap(sigmoid_weights[i].reshape(2, 2), annot=True, fmt='.2f',\n",
    "                cmap='coolwarm', center=0, ax=axes[0, i], cbar=False, vmin=-1, vmax=1)\n",
    "    axes[0, i].set_title(f'Sigmoid - Hidden Node {i}')\n",
    "\n",
    "# ReLU weights\n",
    "relu_weights = model_relu.layer1.weight.data.cpu().numpy()\n",
    "for i in range(3):\n",
    "    sns.heatmap(relu_weights[i].reshape(2, 2), annot=True, fmt='.2f',\n",
    "                cmap='coolwarm', center=0, ax=axes[1, i], cbar=False, vmin=-1, vmax=1)\n",
    "    axes[1, i].set_title(f'ReLU - Hidden Node {i}')\n",
    "\n",
    "plt.suptitle('Learned Weights Comparison', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81d63e0e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stress test comparison\n",
    "stress_tests = {\n",
    "    \"Perfect Horizontal\":  torch.tensor([1.0, 1.0, 0.0, 0.0]),\n",
    "    \"Perfect Vertical\":    torch.tensor([1.0, 0.0, 1.0, 0.0]),\n",
    "    \"Fuzzy Horizontal\":    torch.tensor([0.88, 0.92, 0.12, 0.08]),\n",
    "    \"Fuzzy Vertical\":      torch.tensor([0.91, 0.05, 0.89, 0.11]),\n",
    "    \"Diagonal /\":          torch.tensor([0.0, 1.0, 1.0, 0.0]),\n",
    "    \"Diagonal \\\\\\\\\":         torch.tensor([1.0, 0.0, 0.0, 1.0]),\n",
    "    \"Solid Gray Block\":    torch.tensor([0.5, 0.5, 0.5, 0.5])\n",
    "}\n",
    "\n",
    "print(f\"{'Test Case':<20} | {'Sigmoid H%':<11} | {'Sigmoid V%':<11} | {'ReLU H%':<11} | {'ReLU V%':<11}\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "model_sigmoid.eval()\n",
    "model_relu.eval()\n",
    "\n",
    "# Store results for wandb\n",
    "stress_results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for name, pixels in stress_tests.items():\n",
    "        pixels_device = pixels.to(device)\n",
    "        \n",
    "        pred_sigmoid = model_sigmoid(pixels_device)\n",
    "        pred_relu = model_relu(pixels_device)\n",
    "        \n",
    "        sig_h, sig_v = pred_sigmoid[0].item() * 100, pred_sigmoid[1].item() * 100\n",
    "        relu_h, relu_v = pred_relu[0].item() * 100, pred_relu[1].item() * 100\n",
    "        \n",
    "        print(f\"{name:<20} | {sig_h:>10.1f}% | {sig_v:>10.1f}% | {relu_h:>10.1f}% | {relu_v:>10.1f}%\")\n",
    "        \n",
    "        stress_results.append({\n",
    "            \"test_case\": name,\n",
    "            \"sigmoid_h\": sig_h,\n",
    "            \"sigmoid_v\": sig_v,\n",
    "            \"relu_h\": relu_h,\n",
    "            \"relu_v\": relu_v\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis_swing",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Looking at the stress test results above, notice something concerning:\n",
    "\n",
    "**Perfect/Fuzzy cases (should be confident):**\n",
    "- Perfect Horizontal: ~60-65% (too uncertain!)\n",
    "- Perfect Vertical: ~50-55% (barely better than random!)\n",
    "- Fuzzy cases: ~60% (still too uncertain!)\n",
    "\n",
    "**Ambiguous cases (should be uncertain):**\n",
    "- Diagonals: ~50% ✓ (correct uncertainty)\n",
    "- Gray block: ~50% ✓ (correct uncertainty)\n",
    "\n",
    "### The Problem: Model is Too Conservative\n",
    "\n",
    "**What happened:**\n",
    "- Book notebook: No noise → Memorized → Overconfident (96% on gray blocks!)\n",
    "- Trainer v1: Heavy noise (0.6) → Generalized BUT too cautious (60% on perfect cases)\n",
    "\n",
    "**We swung from one extreme to the other:**\n",
    "\n",
    "```\n",
    "Book: Too confident ──────[Sweet Spot]────── Trainer v1: Too uncertain\n",
    "         (overfit)                                    (over-cautious)\n",
    "```\n",
    "\n",
    "The model learned: \"Everything in training was noisy, so I should never be too sure!\"\n",
    "\n",
    "**This is progress** (at least it's not confidently wrong), but not production-ready.\n",
    "\n",
    "**Next:** The bonus section will show how to find the balance!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064c8a45242",
   "metadata": {},
   "source": [
    "## Part 5: Summary\n",
    "\n",
    "**Training data comparison:**\n",
    "- Discovery: 4 perfect examples\n",
    "- Trainer: ~12,800 noisy examples (64 per epoch × 200 epochs)\n",
    "\n",
    "**Results:** Check wandb dashboard for detailed comparison!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c377a137dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"FINAL COMPARISON: Book vs Trainer\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nTraining Data:\")\n",
    "print(\"  Book Model:       4 perfect examples (no variation)\")\n",
    "print(\"  Trainer Models:   ~12,800 noisy examples (64 per epoch × 200 epochs)\")\n",
    "\n",
    "print(\"\\nStress Test Results:\")\n",
    "print(f\"{'Test Case':<20} | {'Expected':<12} | {'Sigmoid':<12} | {'ReLU':<12}\")\n",
    "print(\"-\" * 72)\n",
    "\n",
    "# Reference expected behaviors\n",
    "expected = {\n",
    "    \"Perfect Horizontal\": \"High H\",\n",
    "    \"Perfect Vertical\": \"High V\",\n",
    "    \"Fuzzy Horizontal\": \"High H\",\n",
    "    \"Fuzzy Vertical\": \"High V\",\n",
    "    \"Diagonal /\": \"Uncertain\",\n",
    "    \"Diagonal \\\\\\\\\": \"Uncertain\",\n",
    "    \"Solid Gray Block\": \"Uncertain\"\n",
    "}\n",
    "\n",
    "# Log to wandb\n",
    "wandb_table_data = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for name, pixels in stress_tests.items():\n",
    "        pixels_device = pixels.to(device)\n",
    "        \n",
    "        pred_sigmoid = model_sigmoid(pixels_device)\n",
    "        pred_relu = model_relu(pixels_device)\n",
    "        \n",
    "        sig_conf = max(pred_sigmoid[0].item(), pred_sigmoid[1].item()) * 100\n",
    "        relu_conf = max(pred_relu[0].item(), pred_relu[1].item()) * 100\n",
    "        \n",
    "        sig_winner = \"H\" if pred_sigmoid[0] > pred_sigmoid[1] else \"V\"\n",
    "        relu_winner = \"H\" if pred_relu[0] > pred_relu[1] else \"V\"\n",
    "        \n",
    "        print(f\"{name:<20} | {expected[name]:<12} | {sig_conf:>5.1f}% {sig_winner:<4} | {relu_conf:>5.1f}% {relu_winner:<4}\")\n",
    "        \n",
    "        wandb_table_data.append([name, expected[name], f\"{sig_conf:.1f}% {sig_winner}\", f\"{relu_conf:.1f}% {relu_winner}\"])\n",
    "\n",
    "# Log results table to wandb\n",
    "wandb.init(project=\"tinynet-trainer\", name=\"final-comparison\", mode=wandb_mode)\n",
    "wandb.log({\n",
    "    \"stress_test_results\": wandb.Table(\n",
    "        columns=[\"Test Case\", \"Expected\", \"Sigmoid\", \"ReLU\"],\n",
    "        data=wandb_table_data\n",
    "    )\n",
    "})\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daba729d159",
   "metadata": {},
   "source": [
    "## Bonus: Finding the Balance\n",
    "\n",
    "**The pendulum swing:**\n",
    "- Discovery: No noise → overfit, 96% confident on gray blocks\n",
    "- Trainer v1: Heavy noise (0.3) → too cautious, 60% on perfect examples\n",
    "- Trainer v2: Let's find the sweet spot with better techniques!\n",
    "\n",
    "**The solution - Multiple improvements:**\n",
    "1. **Mixed batches:** 20% clean + 80% noisy (learn confidence + generalization)\n",
    "2. **Adam optimizer:** Adaptive learning rate (faster, smarter convergence)\n",
    "3. **Weight decay (L2 regularization):** Penalize large weights (prevent overfitting)\n",
    "4. **More epochs:** 300 vs 200 (with regularization, we can train longer)\n",
    "\n",
    "This demonstrates real ML iteration: try → identify weakness → add technique → repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85367378e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mixed_batch(batch_size=64, clean_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Generate batch with mix of clean and noisy examples.\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Total examples to generate\n",
    "        clean_ratio: Fraction of examples that are nearly perfect (0.2 = 20%)\n",
    "    \n",
    "    Strategy: Train on BOTH clear examples (so it learns confidence)\n",
    "    AND noisy examples (so it generalizes)\n",
    "    \"\"\"\n",
    "    clean_size = int(batch_size * clean_ratio)\n",
    "    noisy_size = batch_size - clean_size\n",
    "    \n",
    "    # Clean examples: very low noise\n",
    "    clean_inputs, clean_targets = get_noisy_batch(clean_size, noise_level=0.05)\n",
    "    \n",
    "    # Noisy examples: moderate noise\n",
    "    noisy_inputs, noisy_targets = get_noisy_batch(noisy_size, noise_level=0.3)\n",
    "    \n",
    "    # Combine them\n",
    "    inputs = torch.cat([clean_inputs, noisy_inputs])\n",
    "    targets = torch.cat([clean_targets, noisy_targets])\n",
    "    \n",
    "    return inputs, targets\n",
    "\n",
    "# Test the mixed batch\n",
    "sample_inputs, _ = get_mixed_batch(batch_size=10)\n",
    "print(\"Mixed batch sample (first 10 examples):\")\n",
    "for i in range(10):\n",
    "    pixels = sample_inputs[i].cpu().numpy()\n",
    "    print(f\"  Example {i+1}: [{pixels[0]:.3f}, {pixels[1]:.3f}, {pixels[2]:.3f}, {pixels[3]:.3f}]\")\n",
    "print(\"\\nNotice: Some examples are very clean (1.000, 0.000), others are noisy (0.876, 0.234)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weight_decay_explanation",
   "metadata": {},
   "source": [
    "### What is Weight Decay?\n",
    "\n",
    "**Weight decay (L2 regularization)** adds a penalty for large weights to the loss function:\n",
    "- `Loss_total = Loss_prediction + (weight_decay × sum_of_squared_weights)`\n",
    "\n",
    "**Why it helps:**\n",
    "- Large weights = model is too specific/memorizing\n",
    "- Small weights = model stays general/simple\n",
    "- Penalty encourages the model to find simpler solutions\n",
    "\n",
    "**Effect:** Prevents overfitting even with more training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2b40e9b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize improved ReLU model\n",
    "model_relu_v2 = TinyNet_ReLU().to(device)\n",
    "\n",
    "# Use Adam optimizer (adapts learning rate automatically)\n",
    "optimizer_adam = torch.optim.Adam(model_relu_v2.parameters(), lr=0.01, weight_decay=0.01)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "loss_history_v2 = []\n",
    "\n",
    "# wandb tracking\n",
    "wandb.init(\n",
    "    project=\"tinynet-trainer\",\n",
    "    name=f\"relu-v2-improved-{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "    tags=[\"v2\", \"relu\", \"adam\", \"mixed\"],\n",
    "    mode=wandb_mode,\n",
    "    config={\n",
    "        \"architecture\": \"4-3-2\",\n",
    "        \"activation\": \"relu\",\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"clean_ratio\": 0.2,\n",
    "        \"noisy_level\": 0.3,\n",
    "        \"batch_size\": 64,\n",
    "        \"epochs\": 300,\n",
    "        \"device\": str(device)\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Training improved ReLU model...\")\n",
    "print(\"Improvements: Mixed batches (20% clean) + Adam + Weight decay (0.01) + 300 epochs\\n\")\n",
    "\n",
    "for epoch in range(300):\n",
    "    # Use mixed batches: 30% clean, 70% noisy\n",
    "    inputs, labels = get_mixed_batch(batch_size=64, clean_ratio=0.2)\n",
    "    \n",
    "    outputs = model_relu_v2(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    optimizer_adam.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_adam.step()\n",
    "    \n",
    "    loss_history_v2.append(loss.item())\n",
    "    \n",
    "    # Log to wandb\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": loss.item()})\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch:03d} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "print(f\"\\nFinal loss: {loss_history_v2[-1]:.4f}\")\n",
    "print(f\"Original ReLU loss: {loss_history_relu[-1]:.4f}\")\n",
    "print(f\"Improvement: {((loss_history_relu[-1] - loss_history_v2[-1]) / loss_history_relu[-1] * 100):.1f}% better\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65932cbb62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all three models on stress test\n",
    "print(\"=\" * 95)\n",
    "print(\"STRESS TEST: Original vs Improved\")\n",
    "print(\"=\" * 95)\n",
    "\n",
    "print(f\"\\n{'Test Case':<20} | {'Sigmoid':<15} | {'ReLU v1':<15} | {'ReLU v2 (Improved)':<20}\")\n",
    "print(\"-\" * 95)\n",
    "\n",
    "model_sigmoid.eval()\n",
    "model_relu.eval()\n",
    "model_relu_v2.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for name, pixels in stress_tests.items():\n",
    "        pixels_device = pixels.to(device)\n",
    "        \n",
    "        pred_sigmoid = model_sigmoid(pixels_device)\n",
    "        pred_relu = model_relu(pixels_device)\n",
    "        pred_relu_v2 = model_relu_v2(pixels_device)\n",
    "        \n",
    "        # Get max confidence for each\n",
    "        sig_conf = max(pred_sigmoid[0].item(), pred_sigmoid[1].item()) * 100\n",
    "        relu_conf = max(pred_relu[0].item(), pred_relu[1].item()) * 100\n",
    "        relu_v2_conf = max(pred_relu_v2[0].item(), pred_relu_v2[1].item()) * 100\n",
    "        \n",
    "        sig_winner = \"H\" if pred_sigmoid[0] > pred_sigmoid[1] else \"V\"\n",
    "        relu_winner = \"H\" if pred_relu[0] > pred_relu[1] else \"V\"\n",
    "        relu_v2_winner = \"H\" if pred_relu_v2[0] > pred_relu_v2[1] else \"V\"\n",
    "        \n",
    "        print(f\"{name:<20} | {sig_conf:>5.1f}% {sig_winner:<8} | {relu_conf:>5.1f}% {relu_winner:<8} | {relu_v2_conf:>5.1f}% {relu_v2_winner:<8}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v2_analysis",
   "metadata": {},
   "source": [
    "### Reality Check: Loss vs Behavior\n",
    "\n",
    "**Important observation:** v2 achieved lower loss (~0.247 vs ~0.250) but check the stress test:\n",
    "- Gray block: 96% confident (should be ~50%)\n",
    "- Diagonals: 90%+ confident (should be ~50%)\n",
    "\n",
    "**Lesson:** Loss is not the only metric! A model can:\n",
    "- ✅ Have low loss on training data\n",
    "- ❌ Still be overconfident on ambiguous cases\n",
    "\n",
    "This is why stress testing matters. The bonus section shows further improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3345d527f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all three loss curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(loss_history_sigmoid, label='Sigmoid (200 epochs, SGD)', linewidth=2, alpha=0.8)\n",
    "plt.plot(loss_history_relu, label='ReLU v1 (200 epochs, SGD)', linewidth=2, alpha=0.8)\n",
    "plt.plot(loss_history_v2, label='ReLU v2 (500 epochs, Adam, Mixed)', linewidth=2, alpha=0.8)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Comparison: Iteration Matters!')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=loss_history_relu[-1], color='orange', linestyle='--', alpha=0.3, label='ReLU v1 final')\n",
    "plt.axhline(y=loss_history_v2[-1], color='green', linestyle='--', alpha=0.3, label='ReLU v2 final')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Loss reduction: {loss_history_sigmoid[-1]:.4f} → {loss_history_relu[-1]:.4f} → {loss_history_v2[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9566a640a86",
   "metadata": {},
   "source": [
    "## What We Learned: ML is Iterative\n",
    "\n",
    "**The journey:**\n",
    "\n",
    "**v1 - Sigmoid + Noise:** Loss ~0.250, weak confidence (~50%)\n",
    "- Lesson: Sigmoid saturates with noisy data\n",
    "\n",
    "**v2 - ReLU + Noise:** Loss ~0.217, better but only 63% on perfect cases\n",
    "- Lesson: Improvement, but not production-ready\n",
    "\n",
    "**v3 - ReLU + Mixed Data + Adam + Weight Decay + 300 epochs:** Loss ~0.185, 97% on clear cases\n",
    "- Lesson: Iteration pays off!\n",
    "\n",
    "**Key insights:**\n",
    "1. First attempt rarely works perfectly (that's normal!)\n",
    "2. Data quality matters (mix of clean + noisy beats noisy-only)\n",
    "3. Optimizer choice matters (Adam > SGD for this task)\n",
    "4. Regularization enables longer training (weight decay prevents overfitting)\n",
    "5. More training can help when properly regularized\n",
    "6. Track everything (wandb shows all experiments in one place)\n",
    "\n",
    "**Real ML workflow:**\n",
    "```\n",
    "Train → Evaluate → Identify weakness → Improve → Repeat\n",
    "```\n",
    "\n",
    "This is how professional ML works. You just experienced the real development cycle!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
